---
title: "Choose Your Own Project"
author: "Mark Khusidman"
date: "2023-04-09"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(lubridate)) install.packages("lubridate", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("lubridate", repos = "http://cran.us.r-project.org")
if(!require(quantmod)) install.packages("quantmod", repos = "http://cran.us.r-project.org")
if(!require(glmnet)) install.packages("quantmod", repos = "http://cran.us.r-project.org")

library(tidyverse)
library(caret)
library(lubridate)
library(data.table)
library(quantmod)
library(glmnet)

set.seed(42, sample.kind="Rounding")
```

## Introduction

For reasons that are not difficult to understand, the ability to predict stock price has long been coveted. Although much time has been spent devising techniques to assist in this pursuit, its mastery has proven to be elusive for a number of reasons. The processes underlying changes in a given stock's price are usually both complex and dynamic. Movements in stock price are often subject to a significant amount of noise, especially when viewed at higher granularities. Outliers are also commonly present in price data. Finally, the quantity of data for a particular stock are often limited, and data taken from one stock are rarely applicable when modeling another. Because these inherent difficulties still pose a significant barrier to stock price prediction, further investigation into this subject is warranted. Machine learning represents a relatively new and promising approach to the modeling of stock price. This study aims to test the ability of relatively simple machine learning algorithms, used in combination with basic outlier removal, to model and predict a particular stock's closing price. The stock in question belongs to a video game retailer named Gamestop and trades under the symbol "GME". GME has itself been the subject of a significant amount of attention over the past couple of years due to perceived irregularities in its underlying trading patterns, making it a particularly interesting candidate for exploration. The GME data used in this study have a frequency of 1 observation per day and encompass all of the trading days from March 8th, 2021 through March 8th, 2023, of which there are 505. The data consist of 5 variables: *GME.Open* represents each day's opening price, *GME.Close* represents each day's closing price, *GME.Low* represents the lowest price of each day, *GME.High* represents the highest price of each day, and *GME.Volume* represents the number of GME shares traded on each day. In summary, the initial GME dataset used in this study contains 505 rows of 5 columns. Data modeling is performed using k-nearest-neighbors (k-NN) and elastic net. Each algorithm is used with both an "intact" subset of the GME data and a "clipped" subset which had gone through outlier removal, yielding a total of 4 models to be evaluated.

## Methods

After setting start and end dates, GME data is loaded directly into the environment using the "quantmod" package. The resulting data begins at the start date and encompasses all days up to, but not including, the end date.

```{r, results='hide'}
# Load stock data using quantmod package
start <- as.Date("2021-01-12")
end <- as.Date("2023-05-07")
getSymbols("GME", from = start, to = end)
```

Data with daily frequency are used because this is the highest granularity offered by quantmod. Included in the data is the *GME.Adjusted* column, which should contain the stock's closing price after it is adjusted for various corporate actions. In this case, however, the column is identical to *GME.Close* and is therefore removed. The rest of the data are also checked for missing values.

```{r, results='hold'}
# Drop GME.Adjusted
sprintf("Are GME.Adjusted and GME.Close identical?: %s",
              all(GME$GME.Close == GME$GME.Adjusted))
GME$GME.Adjusted <- NULL

# Make sure there are no missing values in data
sprintf("Any missing values in data?: %s",any(is.na(GME)))
```

This yields an initial dataset which, as previously mentioned, consists of 505 rows and 5 columns. At this point, it is helpful to visualize the data. A candle chart is a common method of visualizing stock data which has the advantage of being able to incorporate all of the columns in the initial dataset.

```{r}
# Visualize the initial data witch a candle chart
candleChart(GME, up.col = "blue", dn.col = "red", theme = "white")
```

Although the individual candles are difficult to make out, the chart still provides a decent sense of how GME's price and volume changed over the associated time-frame. One thing that is immediately clear is that both price and volume are non-stationary. Data is considered non-stationary if either its mean or variance change over time. In this case, both the mean and the variance of all columns in the initial dataset fluctuate over time. This poses a challenge, as non-stationary data is generally more difficult to model with machine learning algorithms than stationary data. Before this issue can be addressed, however, the data must be split into a training set and a test set.

```{r}
# Split data into training and test sets
training <- GME[1: 450,]
test <- GME[451: nrow(GME),]
```

Rather than splitting the data via the removal of a random sample, the initial dataset is split into two contiguous groups. A contiguous set of test data is better than a random sample at approximating the characteristics of future input data, thus leading to a better estimate of a model's generalization error. The training set yielded from this split is pictured below:

```{r}
# Visualize training data 
candleChart(as.xts(training), up.col = "blue", dn.col = "red", theme = "white")
```

Now that the initial dataset has been split, a baseline model can be defined based on the training set. This model will serve as a useful reference point when first evaluating the k-NN and elastic net models. The specific baseline model chosen in this instance is the naive model, also known as the random walk model. Predictions using this model are always equal to the previously observed value, meaning that the predicted series looks like the observed series shifted forward by one time-step. This model is particularly useful for time series with no consistent trend or apparent seasonality. It should be noted that the first 16 values of the training data are not used as observations when evaluating the baseline model. For reasons that will later be made clear, this allows for a better comparison to future evaluations of machine learning models.

```{r}
# Create baseline random walk model for training
observed <- as.numeric(GME$GME.Close[17:450])
baseline_pred_train <- as.numeric(GME$GME.Close[16:449])
baseline_rmse_train <- sqrt(mean((baseline_pred_train - observed)^2))
print(sprintf("Baseline training RMSE: %f",baseline_rmse_train))
```

As shown above, the baseline model yields an RMSE of approximately 2.974.

After the baseline model is defined, preprocessing of the training set begins. First, the aforementioned issue regarding the data's non-stationary nature will be addressed to an extent. This is accomplished by "differencing" the data. This means that every data point is replaced by the difference between said point and its preceding value. It is important to note that this technique only serves to stabilize the data's mean; it is not effective at stabilizing variance.

```{r}
# Calculate differenced training data 
training <- diff(as.matrix(training))
```

The resulting array has one fewer row than before. Now that the data have been differenced, it is useful to again visualize them. A true candle chart of these data cannot be made because the meaning of individual candles would be ambiguous. However, the previously used "candleChart" function can still utilize the differenced data to create a chart which, like before, gives a decent impression of how the data are shaped over time.

```{r}
# Visualize differenced training data 
candleChart(as.xts(training), up.col = "blue", dn.col = "red", theme = "white")
```

Although the differenced data still aren't perfectly stationary, they are far more stationary than the non-differenced data. Next, the training set is standardized. This is done by centering each column before dividing the column by its standard deviation.

```{r}
# Standardize training data
train_sds <- apply(training, 2, sd)
train_means <- colMeans(training)
for(i in 1:5){training[,i] <- (training[,i] - train_means[i]) / train_sds[i]}
```

The training set is subsequently converted into a "data.table" object to ease further preprocessing.

```{r}
# Convert training data to data.table object
training <- as.data.table(training)
```

Before continuing, it is useful to visualize the distribution of values found in the training set. This can be accomplished through the use of histograms.

```{r}
# Visualize distribution of values in training data
training[, 1:5] |> pivot_longer(everything()) |> ggplot(aes(value)) +
  geom_histogram(bins = 35) + facet_wrap(vars(name))
```

All of the columns have roughly normal distributions, although the volume column's peak appears significantly sharper than would be expected for normally distributed data. There also seems to be a right skew in columns which are price-related, although this is less prominent in *GME.Low*. Finally, it can be noted that all columns appear to contain outliers.

Next, 14 lagged columns are created in the training dataset for each of the 5 columns currently present. This means that every row now contains 15 days worth of opening price, closing price, daily high, daily low, and trading volume data.

```{r}
# Add lagged columns to training data
add_lagged <- function(dt, n){
  # Define new column names
  lag_names <- map_chr(1:n, ~ sprintf("lag%d", .))
  lag_names <- expand.grid(lag_names, names(dt))
  lag_names <- apply(lag_names, 1, function(v){paste(v[2], v[1], sep = "_")})
  # Create lagged columns
  dt[, (lag_names) := shift(.SD, 1:n, type = "lag"), .SDcols = names(dt)]
}

add_lagged(training, 14)
```

Once the above code is run, the training dataset contains 75 columns in total. Each row of the current dataset will serve as a single input when the k-NN and elastic net models are trained and given an initial evaluation. Before model training can commence, however, one more column must be defined. The *target* column contains the correct output value for each input. In this case, the target value is the closing price recorded one day after the final date associated with the input. This column will be separated from the inputs before they are used for training or evaluation of machine learning models .

```{r}
# Add target column to training data
training[, target := shift(GME.Close, 1, type = "lead")]
```

It should be noted that the process of adding lagged columns results in 14 rows with at least one empty value, while the addition of the target column adds another such row. These rows must now be removed.

```{r}
# Drop rows with missing values from training data
training <- drop_na(training)
```

At this point, a separate training dataset is created by subjecting the current training data to an outlier removal process. Specifically, rows containing values which are more than 3 standard deviations from their respective mean are removed. Because the training data are already scaled, this is easily done.

```{r}
# Create separate training set with clipped outliers
training_clip <- training[apply(abs(training) <= 3, 1, all),]
```

Note that this operation also removes rows with a target value outside the specified range. From this point forward, the training dataset subjected to outlier removal and its associated models will be referred to as "clipped", while the dataset with no outlier removal and its associated models will be referred to as "intact". The intact training set currently contains 434 rows, while the clipped training set contains 367. Some insight can be gained into the result of the outlier removal process by visualizing the clipped dataset's value distributions. This is done using the same code as before:

```{r}
# Visualize distribution of values in clipped training data
training_clip[, 1:5] |> pivot_longer(everything()) |> ggplot(aes(value)) +
        geom_histogram(bins = 35) + facet_wrap(vars(name))
```

Based on the above histograms, it would appear as though the outlier removal process was successful to a fair extent. The new visualization also reveals a left skew in the volume column which was previously obscured due to binning.

Now that the clipped training set has been created, model tuning can soon begin. As mentioned previously, both the k-NN and elastic net models will be utilized for this purpose. The k-NN algorithm involves first selecting a predefined number of "neighboring" input vectors which are most similar to the input being used for prediction. For continuous data, similarity is generally defined as the euclidean distance between two vectors. The number of similar inputs used for prediction is referred to as *k*. One the most similar inputs are identified, predictions are made by averaging the target values associated with said similar inputs. Because the model makes no underlying assumptions about the relationship between inputs and outputs (apart from the idea that similar inputs will yield similar outputs), it can prove useful in modeling data where this relationship is complex. As the relationship between inputs and outputs is not readily apparent in the GME data, this quality may be useful. Implementing the elastic net model means fitting a regularized linear model to the data. The penalty applied to the parameters is calcualted via a mixture of L1 and L2 normalization. As such, the elastic net model is often thought of as a mixture between Ridge regression and Lasso regression. The extent to which one form of regularization dominates over the other is controlled by the hyperparameter *alpha*. A value close to 0 gives more weight to the L2 penalty, while a value close to 1 give more weight to the L1 penalty. The final penalty value is scaled with the *lambda* hyperparameter, which thereby controls the extent of regularization. Regularized linear models are particularly useful when there is a high degree of colinearity among the input columns. This is because unregularized linear models are prone to undersmoothing when trained on such data. Because there is fairly high degree of colinearity between input columns which are associated with the same time-step, this may be a useful quality. Although the elastic net model ultimately entails fitting a linear model to the data, the inherent generalization involved in such a simplification may offer its own protection against undersmoothing.

One final step to be undertaken before model tuning begins is the creation of a function which turns raw model outputs into closing price predictions. Recall that since the models are being trained on differenced data, their predictions correspond to changes in closing price, rather than the price itself. To create final predictions, each model output must be added to the observed closing value occurring directly before the predicted time-step. Because of the single row lost when differencing the data, the 14 rows lost when creating lagged columns, and the single row lost when adding a target column, the first 16 values of input data are not used as observations when evaluating models.

```{r}
# Convert raw model outputs into closing price predictions during training
get_train_pred <- function(model, lambda = NULL){
  if(is.null(lambda)){
    # If labmda is null, assum k-NN model
    pred <- predict(model, as.matrix(select(training, -target)))
    
  }
  else{
    # If lambda is not null, assume elastic net model
    pred <- predict(model, as.matrix(select(training, -target)),
                    s = lambda)[,1]
  }
  # Unscale raw model outputs
  pred <- (pred * train_sds[1]) + train_means[1]
  # Add unscaled outputs to closing price occuring right before predicted value
  prior_vals <- as.numeric(GME$GME.Close[16:449])
  pred <- prior_vals + pred
  # Define observed values for comparison to predictions
  observed <- GME$GME.Close[17:450]
  
  # Baseline is equal to predictions using the baseline model
  results <- data.frame(pred = pred, observed = as.numeric(observed),  
                        baseline = prior_vals,
                        ind = index(observed))
  results
}
```

The above function yields a data.frame object containing closing price predictions, the observed values corresponding to those predictions, the predictions yielded from the previously defined baseline model, and an index used for visualization purposes. It should be noted that for all models, including clipped ones, evaluation is done using the intact training set. This is done to give the best possible sense of their future testing performance.

Now that the aforementioned function has been defined, model tuning can finally commence. For all 4 models discussed in this study, tuning is conducted via K-fold cross-validation where the number of folds is 10. Like the test data, all 10 validation sets used during the process are contiguous. This makes K-fold cross-validation more representative of this study's final testing conditions than techniques like bootstrapping or Monte-Carlo cross-validation. The intact k-NN model is the first to be tuned.

```{r}
# Tune intact k-NN model
knn_intact <- train(select(training, -target), training[, target],
                trControl = trainControl("cv"),
                tuneGrid = data.frame(list(k = 1:200)),
                method = "knn")
sprintf("Intact k-NN best tune: k = %s", knn_intact$bestTune)
```

The greatest value of *k* investigated during tuning is 200. A limit must be imposed because using too great a fraction of the training data to generate outputs will inhibit variety in the model's predictions. As shown above, the best value of *k* is in this case found to be 128. This can be verified by plotting the relationship between the values of *k* investigated during tuning and their associated RMSEs.

```{r}
# Confirm best value of k for intact k-NN
print(knn_intact$results |> ggplot(aes(k, RMSE)) + geom_line() + geom_point())
```

The above chart seems to confirm that 128 is a reasonable value for *k*.

Next, the tuned model is evaluated on intact training set, yielding an initial impression of its performance.

```{r}
# Evaluate best intact k-NN model on intact training set
knn_train_intact <- get_train_pred(knn_intact$finalModel)
rmse <- sqrt(mean((knn_train_intact$pred - knn_train_intact$observed)^2))
sprintf("Intact k-NN training RMSE: %f", rmse)
```

As shown above, the RMSE of the intact k-NN is approximately 2.954 when evaluated on the intact training set. This represents a slight improvement over the baseline training RMSE of 2.974. A more in-depth assessment of model performance can be made by visually comparing predicted values to observed ones. The chart below represents a subset of the predictions made during the intact k-NN model's initial evaluation. Baseline predictions are also included in the chart for comparison.

```{r}
# Visualize training set predictions yielded by intact k-NN model
print(knn_train_intact[400:434,] |> 
        pivot_longer(cols = c("observed", "pred", "baseline")) |> 
        ggplot(aes(ind, value, color = name)) + geom_line() + geom_point())
```

Based on the above visualization, it is apparent that the predicted values yielded by this model more closely follow the baseline predictions than the observations. However, there are multiple points where the intact k-NN predictions and the baseline predictions significantly differ. The similarity in training RMSE between the baseline model and the intact k-NN model appears to be supported by this visualization.

The clipped k-NN is the second model to be tuned.

```{r, results='hold'}
# Tune clipped k-NN model
knn_clipped <- train(select(training_clip, -target), training_clip[, target],
                     trControl = trainControl("cv"),
                     tuneGrid = data.frame(list(k = 1:200)),
                     method = "knn")
sprintf("Clipped k-NN best tune: k = %s", knn_clipped$bestTune)

# Evaluate best clipped k-NN model on intact training set
knn_train_clipped <- get_train_pred(knn_clipped$finalModel)
rmse <- sqrt(mean((knn_train_clipped$pred - knn_train_clipped$observed)^2))
sprintf("Clipped k-NN training RMSE: %f", rmse)
```

In this case, the best value for *k* is found to be 180, a fairly significant departure from the 128 neighbors used in the intact k-NN model. As with the previous k-NN model, the relationship between the value of *k* and its associated RMSE is visualized.

```{r}
# Confirm best value of k for clipped k-NN
print(knn_clipped$results |> ggplot(aes(k, RMSE)) + geom_line() + geom_point())
```

In this case, it does indeed seem as though 180 is the optimal value of *k* out of those investigated. The clipped k-NN model is now given an initial evaluation. As mentioned previously, both clipped and intact models are evaluated on the intact training set.

```{r}
# Evaluate best clipped k-NN model on intact training set
knn_train_clipped <- get_train_pred(knn_clipped$finalModel)
rmse <- sqrt(mean((knn_train_clipped$pred - knn_train_clipped$observed)^2))
sprintf("Clipped k-NN training RMSE: %f", rmse)
```

Although the clipped k-NN's training RMSE of approximately 2.970 is still marginally better than the baseline of 2.974, it is worse than the intact k-NN's training RMSE of 2.954. The predictions made as part of this model's initial evaluation are visualized in the same manner as the intact k-NN's training set predictions.

```{r}
# Visualize training set predictions yielded by clipped k-NN model
knn_train_clipped[400:434,] |> 
        pivot_longer(cols = c("observed", "pred", "baseline")) |> 
        ggplot(aes(ind, value, color = name)) + geom_line() + geom_point()
```

The predictions made by the clipped k-NN model follow the baseline predictions more closely than the intact k-NN model. However, some clipped k-NN predictions are still noticeably different from their associated baseline predictions. Based on the above visualization, it is easy to see why the training RMSE of the clipped k-NN model is so similar to that of the baseline model.

The intact elastic net model is the third to be tuned. It should be noted that the *alpha* and *lambda* hyperparameters are tuned separately: First, the value for *alpha* is found using the "caret" package. Next, the "glmnet" package is used directly to find the value of *lambda* while the value of *alpha* remains fixed. This is done because glmnet's default tuning implementation finds the optimal *lambda* by testing against an extensive, non-linear sequence of values. Although it would have been possible to reconstruct this sequence and use it with caret's "train" function, this would likely have been time-consuming.

```{r}
# Tune intact elastic net model
enet_intact <- train(select(training, -target), training[, target],
               trControl = trainControl("cv"),
               method = "glmnet")
best_alpha_intact <- enet_intact$bestTune[,"alpha"]

# Use in-depth search to find best lambda for intact elastic net
lambda_cv_intact <- cv.glmnet(as.matrix(select(training, -target)), training[, target], alpha = best_alpha_intact)

best_lambda_intact <- lambda_cv_intact$lambda.min

sprintf("Intact elastic net best tune: alpha = %s, lambda = %s", 
              enet_intact$bestTune[1], best_lambda_intact)
```

In this case, the optimal value of *alpha* is found to be 1. This means that the intact elastic net model is functionally equivalent to a Lasso regression model. The optimal value of lambda in this instance is found to be around 0.0995. Next, the intact elastic net model is given its initial evaluation.

```{r}
# Evaluate best intact elastic net model on intact training set
enet_train_intact <- get_train_pred(enet_intact$finalModel, 
                                    lambda = best_lambda_intact)
rmse <- sqrt(mean((enet_train_intact$pred - enet_train_intact$observed)^2))
sprintf("Intact elastic net training RMSE: %f", rmse)
```

The training RMSE yielded by the intact elastic net model is found to be identical to the baseline training RMSE of 2.974267. As before, the model's predictions are visualized.

```{r}
# Visualize training set predictions yielded by intact elastic net model
print(enet_train_intact[400:434,] |> 
        pivot_longer(cols = c("observed", "pred", "baseline")) |> 
        ggplot(aes(ind, value, color = name)) + geom_line() + geom_point())
```

The above chart demonstrates that the intact elastic net model's predictions appear identical to their associated baseline model predictions. This must mean that all raw outputs yielded by the model, which represent changes in closing price rather than the price itself, are close to zero. This would make the intact elastic net model functionally similar to the baseline model, thereby yielding a similar or identical RMSE when evaluated.

The clipped elastic net model is the fourth and final model to be tuned.

```{r}
# Tune clipped elastic net model
enet_clipped <- train(select(training_clip, -target), training_clip[, target],
                trControl = trainControl("cv"),
                method = "glmnet")
best_alpha_clipped <- enet_clipped$bestTune[,"alpha"]

# Use in-depth search to find best lambda for clipped elastic net
lambda_cv_clipped <- cv.glmnet(as.matrix(select(training_clip, -target)), training_clip[, target], alpha = best_alpha_clipped)

best_lambda_clipped <- lambda_cv_clipped$lambda.min

sprintf("Clipped elastic net best tune: alpha = %s, lambda = %s", 
              enet_clipped$bestTune[1], best_lambda_clipped)
```

As in the case of the intact elastic net model, the value of *alpha* is chosen to be 1. This means that, like the intact version, the clipped elastic net model functions as a Lasso regression model. The optimal value of *lambda* in this case is found to be around 0.0579, representing an approximately 42% decrease when compared to the intact elastic net model's optimal *lambda*. The initial evaluation of this model is now performed.

```{r}
# Evaluate best clipped elastic net model on intact training set
enet_train_clipped <- get_train_pred(enet_clipped$finalModel, 
                                     lambda = best_lambda_clipped)
rmse <- sqrt(mean((enet_train_clipped$pred - enet_train_clipped$observed)^2))
sprintf("Clipped elastic net training RMSE: %f", rmse)
```

The yielded training RMSE of approximately 2.974312 is the worst one out the 4 collected, albeit by a very small margin. It is the only training RMSE found to be greater than the baseline training RMSE. As with the k-NN models, this study's outlier removal process appears to cause an increase in the training RMSE of elastic net models. The clipped elastic net model's training data predictions are now visualized as was done for the other models.

```{r}
# Visualize training set predictions yielded by clipped elastic net model
enet_train_clipped[400:434,] |> 
        pivot_longer(cols = c("observed", "pred", "baseline")) |> 
        ggplot(aes(ind, value, color = name)) + geom_line() + geom_point()
```

Although the clipped elastic net predictions follow their associated baseline predictions almost perfectly, slight differences can be detected at all points. Unlike what was seen with the k-NN models, predictions yielded by the clipped elastic net model are less similar to the baseline predictions than are the intact elastic net predictions. Now that that evaluation and visualization of all 4 models' performance on the training data has been completed, the models will soon be evaluated on the the test set. Although only the k-NN models outperformed the baseline model when evaluated on the intact training set, all will be investigated further for informative purposes. Before this can be done, however, the test data must be preprocessed in the same manner as the intact training data.

```{r}
# Difference test data
test <- diff(as.matrix(test))
# Scale test data using means and sds from training data
for(i in 1:5){test[,i] <- (test[,i] - train_means[i]) / train_sds[i]}

test <- as.data.table(test)
# Add lagged columns to test data
add_lagged(test, 14)
test <- drop_na(test)
```

It is important to note that no *target* column is added to the test data. Additionally, rather than scaling the testing columns with their own means and standard deviations as was done previously, the test data are instead scaled using the means and standard deviations calculated from the unscaled training set. It is important for new data presented to the models to be in the same scale as the data used to train them. Next, it is worthwhile to create a testing baseline RMSE by applying the baseline model to the test data. This is done to put testing results into a more useful context.

```{r}
# Create baseline model for testing
observed <- as.numeric(GME$GME.Close[467:583])
baseline_pred_test <- as.numeric(GME$GME.Close[466:582])
baseline_rmse_test <- sqrt(mean((baseline_pred_test - observed)^2))
print(sprintf("Baseline test RMSE: %f",baseline_rmse_test))
```

The baseline testing RMSE of approximately 1.118 is much smaller than the baseline training RMSE. This is likely because the testing data has a lower variance than the training data. The final step before the models can be evaluated on the test set is the creation of a new function to handle raw model outputs. It is essentially the same as the function used when handling training data predictions; The only differences are that the final prediction is removed (this was not necessary during training due to the addition of the *target* column), a different set of closing values are used for addition with raw model outputs, and some minor syntactical changes stemming from the lack of a *target* column in the testing data.

```{r}
# Convert raw model outputs into closing price predictions during testing
get_test_pred <- function(model, lambda = NULL){
  if(is.null(lambda)){
    pred <- predict(model, as.matrix(test))
    
  }
  else{
    pred <- predict(model, as.matrix(test),
                    s = lambda)[,1]
  }
  # Remove last prediction due to lack of corresponding observation
  pred <- pred[1: length(pred) - 1]
  pred <- (pred * train_sds[1]) + train_means[1]
  prior_vals <- as.numeric(GME$GME.Close[466:582])
  pred <- prior_vals + pred
  observed <- GME$GME.Close[467:583]
  
  results <- data.frame(pred = pred, observed = as.numeric(observed),  
                        baseline = prior_vals,
                        ind = index(observed))
  results
}
```

## Results

The models are evaluated on the test data in a very similar manner to their evaluation on the training data. First to be evaluated is the intact k-NN model.

```{r}
knn_test_intact <- get_test_pred(knn_intact$finalModel)
rmse <- sqrt(mean((knn_test_intact$pred - knn_test_intact$observed)^2))
sprintf("Intact k-NN test RMSE: %f", rmse)
```

The yielded RMSE of approximately 1.138 is worse than the baseline testing RMSE. The model's performance is further examined with a visualization as was done before. The below chart encompasses all of the testing data.

```{r}
# Visualize intact k-NN model on testing data
knn_test_intact |> pivot_longer(cols = c("observed", "pred", "baseline")) |> 
  ggplot(aes(ind, value, color = name)) + geom_line() + geom_point()
```

As seen in the intact k-NN model's initial evaluation, model predictions follow baseline predictions more than observed values, although noticable differences between model predictions and baseline predictions still exist. It appears as though their is less of a difference between the intact k-NN model's predictions and baseline predictions than what was seen in the initial evaluation.

Next to be evaluated on the test set is the clipped k-NN model.

```{r}
# Evaluate clipped k-NN model on testing data
knn_test_clipped <- get_test_pred(knn_clipped$finalModel)
rmse <- sqrt(mean((knn_test_clipped$pred - knn_test_clipped$observed)^2))
sprintf("Clipped k-NN test RMSE: %f", rmse)
```

As in the previous case, the yielded RMSE of around 1.1267 is worse than the baseline testing RMSE. Unlike what was seen during initial evaluations, however, the clipped k-NN model yields a smaller error than the intact version. This model's testing predictions are visualized below.

```{r}
# Visualize clipped k-NN model on testing data
knn_test_clipped |> pivot_longer(cols = c("observed", "pred", "baseline")) |> ggplot(aes(ind, value, color = name)) + geom_line() + geom_point()
```

As was noted during initial evaluations, the clipped k-NN model's predictions follow the baseline prediction more closely than predictions from the intact k-NN model. Like the intact k-NN model, the clipped version seems to follow the baseline model more closely during testing than it did during initial evaluations.

The intact elastic net is the third model to be tested.

```{r}
# Evaluate intact elastic net model on testing data
enet_test_intact <- get_test_pred(enet_intact$finalModel, 
                                  lambda = best_lambda_intact)
rmse <- sqrt(mean((enet_test_intact$pred - enet_test_intact$observed)^2))
sprintf("Intact elastic net test RMSE: %f", rmse)
```

The yielded RMSE of 1.118236 is marginally better than the baseline testing RMSE 1.118241, although this is unlikely to be significant. Unlike what was seen during initial evaluations, the intact elastic net model performs better than the intact k-NN model. As in the previous cases, the model's predictions are shown below.

```{r}
# Visualize intact elastic net model on testing data
enet_test_intact |> pivot_longer(cols = c("observed", "pred", "baseline")) |> ggplot(aes(ind, value, color = name)) + geom_line() + geom_point()
```

As was seen during initial evaluations, the intact elastic net model's predictions appear indistinguishable baseline predictions, suggesting that the model's raw outputs are close to zero. Although the slight discrepancy between the intact elastic net model's RMSE and the baseline testing RMSE indicates that the two sets of predictions are not completely identical, this cannot be visually confirmed.

The final model to be evaluated on the test set is the clipped elastic net model.

```{r}
# Evaluate clipped elastic net model on testing data
enet_test_clipped <- get_test_pred(enet_clipped$finalModel, 
                                   lambda = best_lambda_clipped)
rmse <- sqrt(mean((enet_test_clipped$pred - enet_test_clipped$observed)^2))
sprintf("Clipped elastic net test RMSE: %f", rmse)
```

The yielded RMSE of approximately 1.120 is larger than the baseline testing RMSE. As was seen during initial evaluations, the clipped elastic net model demonstrated slightly worse performance than the intact version. The model's predictions are visualized below.

```{r}
# Visualize clipped elastic net model on testing data
enet_test_clipped |> pivot_longer(cols = c("observed", "pred", "baseline")) |> ggplot(aes(ind, value, color = name)) + geom_line() + geom_point()
```

As was noted during initial evaluations, the clipped elastic net model's predictions follow the baseline predictions extremely closely, but not perfectly. Also like what was seen earlier, the clipped elastic net model's predictions differ more from the baseline predictions than do the intact elastic net model's predictions, again running counter to what is observed when the k-NN models are compared.

## Conclusion

Before the final performance of the ML models is discussed, there are are a few observations regarding their behavior worth mentioning. It would appear as though none of the 4 models explored in this study are capable of significantly outperforming the baseline random walk model when evaluated on the test data. Although the intact elastic net model testing RMSE is technically smaller than the baseline testing RMSE, the difference between them is likely insignificant. The potential effectiveness of all models is likely reduced due to the large difference in variance between the training data and the test data. The non-stationary relationship between variance and time within the training data likely interferes with successful modeling as well. Another factor contributing to successful modeling is the high degree of noise present in the differenced training data. This is a known drawback of making the data more stationary via differencing. Another clear limitation of the present study was the relatively small quantity of data used. This may be addressed in the future when more data is available. Finding GME data with a higher frequency could also potentially mitigate this issue. It should also be noted that the outlier removal method used in this study is very basic. The use of a more advanced outlier removal method such as LOF may serve to improve results in future research. Another limitation in this study is that only 2 types of machine learning models were explored. Future research should consider a wider array of model types.
